<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>feature engineering on Artistic Portfolio of Chris Ried</title>
    <link>https://chrisried.xyz/tags/feature-engineering/</link>
    <description>Recent content in feature engineering on Artistic Portfolio of Chris Ried</description>
    <image>
      <url>https://chrisried.xyz/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://chrisried.xyz/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 08 Oct 2017 00:00:00 +0000</lastBuildDate><atom:link href="https://chrisried.xyz/tags/feature-engineering/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Tips on Feature Engineering</title>
      <link>https://chrisried.xyz/posts/2017-10-08-feature-engineering/</link>
      <pubDate>Sun, 08 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://chrisried.xyz/posts/2017-10-08-feature-engineering/</guid>
      <description>Tips on Feature Engineering to fit how classifiers work; giving a geometry problem to a tree, oversized dimension to a kNN and interval data to an SVM are not a good ideas remove as much nonlinearities as possible; expecting that some classifier will do Fourier analysis inside is rather naive (even if, it will waste a lot of complexity there) make features generic to all objects so that some sampling in the chain won&amp;rsquo;t knock them out check previous works &amp;ndash; often transformation used for visualisation or testing similar types of data is already tuned to uncover interesting aspects avoid unstable, optimizing transformations like PCA which may lead to overfitting experiment a lot </description>
    </item>
    
  </channel>
</rss>
